{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Skript: Webscraping mit BeautifulSoup\n",
    "\n",
    "Benötigt werden:\n",
    "\n",
    "* Pakete `requests` und `bs4`\n",
    "\n",
    "* Grundverständnis von HTML\n",
    "\n",
    "### Achtung!\n",
    "\n",
    "Vor jedem Webscraping stellen wir uns folgende Fragen:\n",
    "- Gibt es auf der Webseite stattdessen eine API, auf welche wir zugreifen können? Das erspart uns Arbeit.\n",
    "- Verbietet die Webseite das Scrapen von Daten? (`/robots.txt` überprüfen)\n",
    "\n",
    "## Beautiful Soup 4\n",
    "\n",
    "Dieses Modul kann HTML-Text auslesen und alle Informationen extrahieren. Wir können aus einem  BeautifulSoup-Objekt dann die Schnipsel holen, die für uns Wert haben. Das Modul müssen wir zunächst installieren. Achtung! Installiert wird es unter dem Namen `beautifulsoup4`, importiert wird es jedoch unter `bs4`! Wir können conda für die Installation nutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modulimporte\n",
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel HTML:\n",
    "html = '''<section><article><h1>Test-Artikel</h1><p>Dies ist ein Beispiel-Artikel in HTML-Form.</p><p>Absatz 2.</p></article></section>'''\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwendung bs4: Suppen-Objekt erstellen\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was kann unser Objekt alles?\n",
    "dir(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie sieht der Inhalt aus?\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe-Formatierung mit .prettify() \"verschönern\"\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir können auch spezielle Formatierer\n",
    "# \"Formatter\" benutzen, die uns den Inhalt\n",
    "# schön darstellen.\n",
    "formatter = bs4.formatter.HTMLFormatter(indent=4)\n",
    "print(soup.prettify(formatter=formatter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wichtigste Funktionen:\n",
    "# find - Findet den ersten Eintrag\n",
    "# des angegebenen HTML Tags\n",
    "soup.find(\"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Alternative Notation: \n",
    "soup.h1"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "soup.find(\"p\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "soup.p"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Find ist mächtiger als die Punkt-Notation, da man \n",
    "# hier spezifischer werden kann."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all - Findet alle Einträge\n",
    "# einer Art von HTML-Tag (und gibt sie in Liste zurück)\n",
    "soup.find_all(\"p\")\n",
    "# Einstellbar über Parameter \"name\", \"attrs\" und \"class_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all gibt Liste mit Treffern zurück\n",
    "# Einzelelemente extrahierbar\n",
    "soup.find_all(\"p\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Iteration möglich:\n",
    "paragraph_soup = soup.find_all(\"p\")\n",
    "\n",
    "for paragraph in paragraph_soup:\n",
    "    print(paragraph)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nur Text, ohne Tags ausgeben\n",
    "soup.find_all(\"p\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "paragraph_soup = soup.find_all(\"p\")\n",
    "\n",
    "for paragraph in paragraph_soup:\n",
    "    print(paragraph.text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Webscraping aus einer lokalen HTML-Datei\n",
    "# Bisschen Zusatz-Infos: https://wiki.selfhtml.org/wiki/HTML/Tabellen/Aufbau_einer_Tabelle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Reinladen, die gute Datei!\n",
    "with open('Liste der größten Schiffe der Welt – Wikipedia.html', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "# bs4 hat mehrere Parser, die Web-Dokumente scannen.\n",
    "# Die Wahl des passenden Parsers hängt von den Anforderungen ab und sie stellen das\n",
    "# Dokument unterschiedlich dar. Wir benutzen im Folgenden den html-Parser.\n",
    "# Mehr Infos:\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Suppe bestaunen:\n",
    "soup"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Die Tabelle heraussondern:\n",
    "soup.find('table')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Selbes Element, anderer Weg:\n",
    "soup.table"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "soup.table == soup.find('table')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Wenn's hübscher aussehen soll:\n",
    "print(soup.find('table').prettify())"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Alle Datenfelder holen:\n",
    "fields = soup.find('table').find_all('td')\n",
    "fields"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Nur die Text-Inhalte der Datenfelder:\n",
    "for field in fields:\n",
    "    print(field.text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Alle Links sammeln:\n",
    "soup.find('table').find_all('a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Was ist eigentlich in dieser Liste drin?\n",
    "for element in soup.find('table').find_all('a'):\n",
    "    print(type(element))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Packen wir doch das Zwischenergebnis für die Lesbarkeit auf eine Variabel:\n",
    "table_links = soup.find('table').find_all('a')\n",
    "table_links"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Wir wollen JEDEN Link aus dieser Liste bekommen, starten aber erstmal klein\n",
    "# mit nur einem Link: \n",
    "table_links[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Man kann auf Attribute mit Schlüssel-Notation zugreifen (kennen wir von Dicts):\n",
    "table_links[0]['href']"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Alternativ geht das auch mit der get-Notation:\n",
    "table_links[0].get('href')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Und jetzt Denkschmalz-Aufgabe: Wie lasse ich mir ALLE Links ausgeben?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Natürlich müssen wir für unsere Suppen keine lokale HTML-Datei vorliegen haben"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Schiffe mit requests an Land ziehen:\n",
    "ships_url = 'https://de.wikipedia.org/wiki/Liste_der_gr%C3%B6%C3%9Ften_Schiffe_der_Welt'\n",
    "response = requests.get(ships_url, 'html.parser')\n",
    "ship_soup = BeautifulSoup(response.text)\n",
    "ship_soup.find('table')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Und jetzt seid ihr dran! Nehmt DataCraft und holt von dort die Namen \n",
    "# und Background der Dozenten! URL: https://www.data-craft.de/\n",
    "# Bonus: Regelt die Sache mit dem Encoding. ;)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping am Beispiel erklärt\n",
    "\n",
    "Als Beispiel untersuchen wir die Webseite https://worldofwarcraft.com/de-de/game/classes\n",
    "\n",
    "Um zu prüfen, was wir auf der Webseite dürfen und ob bestimmte Inhalte von Webscraping verboten sind, müssen wir die robots.txt Seite aufsuchen\n",
    "https://worldofwarcraft.com/robots.txt\n",
    "\n",
    "Den Inhalt der Webseite können wir schon in python abrufen. Dazu können wir `requests.get` verwenden. Wir erhalten allerdings HTML Code zurück, und müssen diesen durchsuchen, um an die Infos zu kommen, die uns interessieren. Dafür benutzen wir \"Beautiful Soup 4\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Schritt 1: Webseiten Inhalte abrufen\n",
    "url = 'https://worldofwarcraft.com/de-de/game/classes'\n",
    "response = requests.get(url)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Suppen-Objekt erstellen -> Inhalt wird von bs4 ausgelesen und umgewandelt\n",
    "results = BeautifulSoup(\n",
    "    response.text,  \n",
    "    'html.parser'  # Angabe, welche Art von Inhalt ausgelesen wird (XML wird auch unterstützt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Schritt 3: Die Suppe durchsuchen\n",
    "#            Welche HTML-Tags wollen wir finden?\n",
    "\n",
    "\n",
    "# <div class=\"Card-title\">\n",
    "classes = results.find_all(name=\"div\", class_=\"Card-title\")\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Schritt 4: Jetzt wollen wir für jedes Objekt nur den\n",
    "# Textinhalt und diesen als neue Liste speichern.\n",
    "classes = {'Klasse': [cl.text for cl in classes]}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 5: Gesammelte Daten optional zu einem\n",
    "# DataFrame umwandeln.\n",
    "classes_df = pd.DataFrame(classes)\n",
    "classes_df"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Aufgabe: \n",
    "# Holt Euch die Zitate und Autorennamen von folgender URL:\n",
    "# https://quotes.toscrape.com/\n",
    "# Lasst Euch alle Zitate mit Autoren ausgeben oder schreibt diese in eine txt-Datei!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
